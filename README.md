# ML-Features-Deployment
Code and notes on feature engineering, feature selection, and model deployment for full-stack data science.

### Project Breakdown
1. Feature Engineering
2. Feature Selection
3. Model Depeloyment

### Notes on Feature Engineering
- High-level overivew
  - Variables Types (Numerical, Categorical, Dates, and Mixed)
  - Variable Characterstics (Missing data, Cardinality, Frequency of Labels, Distributions, Outliers, Feature Magnitude) 
  - Missing Data Imputation (Mean/Median/Mode Imputation, Random Sample imputation, Arbitrary Value Imputation, Missing Indicator, Regression, MICE)
  - Categorical Encoding (One Hot Encoding, Ordinal Encoding, Count/Frequency Encoding, Mean/Target, Weight of Evidence, Rare Label, Shashing)
  - Variable transformation (Logarthmic,m Power, Box-Cox, Yeo-Johnson)
  - Discretisation (Equal-width, Equal-Frequncy, Discretation Decision Trees/Clustering)
  - Outliers (Identifcation, Removal, Capping, Winsorization)
  - Dates (Extract Year, Month, Day, Week, Time Zone)
  - Feature Scaling (Standardisation, MixMaxScaling, Robust, Maximum Absolute)
  - Feature Engineering (Advantages & Limitations, Underlying Assumptions, Pandas & Numpy, Scikit-Learn)
  - Semi-Automated Pipeline 
- Background
  - Linear and Logistic Regression
  - Decision Trees & Random Forest
  - Gradient Boosted Trees
  - Nearest Neighbours
  - Model Diagnostics: ROC-AUC, Accuracy, MSE, RMSE, etc.
  - Numpy & Pandas, Sci-Kit Learn, Matplotlib, Seaborn
  - Python >= 3.7 Installation
  - Feature-Engine
  - Kaggle Datasets 
